{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tooth007 = 'https://osf.io/7trgk/download'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tooth045 = 'https://osf.io/fqm4x/download'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Root = 'data'\n",
    "os.makedirs(Root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(Root, '7.zip')):\n",
    "    print('File already downloaded')\n",
    "else:\n",
    "    # Progress bar for download via https://stackoverflow.com/a/44920494/323100\n",
    "    download = requests.get(tooth007, stream=True)\n",
    "    totalsize = int(download.headers[\"Content-Length\"])\n",
    "    chunksize = 1024 * 1024\n",
    "    bars = int(totalsize / chunksize)\n",
    "    with open(os.path.join(Root, '7.zip'), \"wb\") as f:\n",
    "        for chunk in notebook.tqdm(download.iter_content(chunk_size=chunksize),\n",
    "                                   total=bars,\n",
    "                                   unit=\"MB\",\n",
    "                                   desc='Downloading 7.zip (%s GB)' % round(totalsize * 1e-9,1)):        \n",
    "            f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(Root, '45.zip')):\n",
    "    print('File already downloaded')\n",
    "else:    \n",
    "    # Progress bar for download via https://stackoverflow.com/a/44920494/323100\n",
    "    download = requests.get(tooth045, stream=True)\n",
    "    totalsize = int(download.headers[\"Content-Length\"])\n",
    "    chunksize = 1024 * 1024\n",
    "    bars = int(totalsize / chunksize)\n",
    "    with open(os.path.join(Root, '45.zip'), \"wb\") as f:\n",
    "        for chunk in notebook.tqdm(download.iter_content(chunk_size=chunksize),\n",
    "                                   total=bars,\n",
    "                                   unit=\"MB\",\n",
    "                                   desc='Downloading 45.zip (%s GB)' % round(totalsize * 1e-9,1)):        \n",
    "            f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ziplist = glob.glob(os.path.join(Root, '*.zip'))\n",
    "for file in notebook.tqdm(ziplist,\n",
    "                          total=len(ziplist),\n",
    "                          desc='Unzipping %s files' % len(ziplist)):\n",
    "    with zipfile.ZipFile(file, 'r') as zip_file:\n",
    "        for file in notebook.tqdm(iterable=zip_file.namelist(),\n",
    "                                  desc=file,\n",
    "                                  total=len(zip_file.namelist()),\n",
    "                                  leave=False):\n",
    "            if not os.path.exists(os.path.join(Root, file)):\n",
    "                zip_file.extract(member=file, path=Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Folder'] = glob.glob(os.path.join('data', '*' + os.path.sep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['LogFile'] = [sorted(glob.glob(os.path.join(f, '*.log')))[0] for f in Data['Folder']]\n",
    "print('We have %s tooth samples to work with' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Sample'] = [os.path.splitext(os.path.basename(l))[0] for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Reconstructions'] = [sorted(glob.glob(os.path.join(f,\n",
    "                                                         'rec',\n",
    "                                                         '*rec*.png'))) for f in Data['Folder']]\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check voxel sizes (*rounded* to two after-comma values)\n",
    "# If different, spit out which values\n",
    "if len(Data['Voxelsize'].round(2).unique()) > 1:\n",
    "    print('We scanned the teeht with %s different voxel sizes' % len(Data['Voxelsize'].round(2).unique()))\n",
    "    for vs in sorted(Data['Voxelsize'].round(2).unique()):\n",
    "        print('-', vs, 'um for Samples ', end='')\n",
    "        for c, row in Data.iterrows():\n",
    "            if float(vs) == round(row['Voxelsize'], 2):\n",
    "                print(row.Sample, end=', ')\n",
    "        print('')\n",
    "else:\n",
    "    print('We scanned all %s datasets with equal voxel size, namely %s.' % (len(Data),\n",
    "                                                                            Data['Voxelsize'].unique()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['PreviewImagePath'] = [sorted(glob.glob(os.path.join(f, 'rec', '*.bmp'))) for f in Data['Folder']]\n",
    "Data['PreviewImage'] = [imageio.imread(pip[0])\n",
    "                        if pip\n",
    "                        else numpy.random.random((100, 100)) for pip in Data['PreviewImagePath']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    '''\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    '''\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git',\n",
    "                        '--git-dir',\n",
    "                        os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse',\n",
    "                        '--short',\n",
    "                        '--verify',\n",
    "                        'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory for output\n",
    "OutPutDir = os.path.join(os.getcwd(), 'Output', get_git_hash())\n",
    "print('We are saving all the output to %s' % OutPutDir)\n",
    "os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "# Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')\n",
    "# Resize figures\n",
    "plt.rcParams['figure.figsize'] = (16, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "    plt.imshow(row.PreviewImage)\n",
    "    plt.title(row.Sample)\n",
    "    plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um', color='black'))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5)\n",
    "plt.savefig(os.path.join(OutPutDir, 'ScanOverviews.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_image.imread\n",
    "from numcodecs import Blosc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all reconstructions into a DASK array and save them to disk\n",
    "# Partially based on http://stackoverflow.com/a/39195332/323100\n",
    "# and on /LungMetastasis/HighResolutionScanAnalysis.ipynb\n",
    "Data['OutputNameRec'] = [os.path.join(f, sample + '_rec.zarr') for f, sample in zip(Data.Folder, Data.Sample)]\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Converting reconstructions to .zarr',\n",
    "                            total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameRec']):\n",
    "        print('%3s/%3s: Reading %s reconstructions and saving to %s' % (c + 1,\n",
    "                                                                        len(Data),\n",
    "                                                                        row['Number of reconstructions'],\n",
    "                                                                        row['OutputNameRec'][len(Root):]))\n",
    "        Reconstructions = dask_image.imread.imread(os.path.join(row['Folder'], 'rec', '*rec*.png'))\n",
    "        Reconstructions.to_zarr(row['OutputNameRec'],\n",
    "                                overwrite=True,\n",
    "                                compressor=Blosc(cname='zstd',\n",
    "                                                 clevel=3,\n",
    "                                                 shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reconstructions = [dask.array.from_zarr(file) for file in Data['OutputNameRec']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Size'] = [rec.shape for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = ['Axial',\n",
    "              'Coronal',\n",
    "              'Sagittal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the middle slices, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Mid_' + direction] = [None] * len(Reconstructions)\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Middle images', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'], '%s.Middle.%s.png' % (row['Sample'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][Data['Size'][c][0] // 2].compute()\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][:, Data['Size'][c][1] // 2, :].compute()\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][:, :, Data['Size'][c][2] // 2].compute()\n",
    "            # Save the calculated 'direction' view to disk\n",
    "            imageio.imwrite(outfilepath, (Data.at[c, 'Mid_' + direction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle slices\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Saving middle images overview', total=len(Data)):\n",
    "    outfilepath = os.path.join(row['Folder'], row['Sample'] + '.MiddleSlices.png')\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['Mid_' + direction])\n",
    "        if d == 0:\n",
    "            plt.axhline(row.Size[1] // 2, c=seaborn.color_palette()[0])\n",
    "            plt.axvline(row.Size[2] // 2, c=seaborn.color_palette()[1])\n",
    "            plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um', color=seaborn.color_palette()[2]))\n",
    "        elif d == 1:\n",
    "            plt.axhline(row.Size[0] // 2, c=seaborn.color_palette()[2])\n",
    "            plt.axvline(row.Size[d] // 2, c=seaborn.color_palette()[1])\n",
    "            plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um', color=seaborn.color_palette()[0]))\n",
    "        else:\n",
    "            plt.axhline(row.Size[0] // 2, c=seaborn.color_palette()[2])\n",
    "            plt.axvline(row.Size[d] // 2, c=seaborn.color_palette()[0])\n",
    "            plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um', color=seaborn.color_palette()[1]))\n",
    "        plt.title('%s, %s' % (row['Sample'],\n",
    "                              direction + ' Middle slice'))\n",
    "        plt.axis('off')\n",
    "    if not os.path.exists(outfilepath):        \n",
    "        plt.savefig(outfilepath, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = [None] * len(Reconstructions)\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='MIPs', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'], '%s.MIP.%s.png' % (row['Sample'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'MIP_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c, 'MIP_' + direction] = Reconstructions[c].max(axis=d).compute()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c, 'MIP_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show MIP slices\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Saving middle images overview', total=len(Data)):\n",
    "    outfilepath = os.path.join(row['Folder'], row['Sample'] + '.MIPs.png')\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['MIP_' + direction])\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "        plt.title('%s, %s' % (row['Sample'],\n",
    "                              direction + ' MIP'))\n",
    "        plt.axis('off')\n",
    "    if not os.path.exists(outfilepath):\n",
    "        plt.savefig(outfilepath, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
